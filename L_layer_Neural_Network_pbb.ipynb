{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (5.0, 4.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: initialize_parameters_deep\n",
    "\n",
    "def initialize_parameters_deep(layer_dims):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    layer_dims -- python array (list) containing the dimensions of each layer in our network\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your parameters \"W1\", \"b1\", ..., \"WL\", \"bL\":\n",
    "                    Wl -- weight matrix of shape (layer_dims[l], layer_dims[l-1])\n",
    "                    bl -- bias vector of shape (layer_dims[l], 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(3)\n",
    "    parameters = {}\n",
    "    L = len(layer_dims)            # number of layers in the network\n",
    "\n",
    "    for l in range(1, L):\n",
    "        ### START CODE HERE ### (≈ 2 lines of code)\n",
    "        parameters['W' + str(l)] = np.random.randn(layer_dims[l],layer_dims[l-1])*0.01\n",
    "        parameters['b' + str(l)] = np.zeros((layer_dims[l],1))\n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "        assert(parameters['W' + str(l)].shape == (layer_dims[l], layer_dims[l-1]))\n",
    "        assert(parameters['b' + str(l)].shape == (layer_dims[l], 1))\n",
    "\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: linear_forward\n",
    "\n",
    "def linear_forward(A, W, b):\n",
    "    \"\"\"\n",
    "    Implement the linear part of a layer's forward propagation.\n",
    "\n",
    "    Arguments:\n",
    "    A -- activations from previous layer (or input data): (size of previous layer, number of examples)\n",
    "    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
    "    b -- bias vector, numpy array of shape (size of the current layer, 1)\n",
    "\n",
    "    Returns:\n",
    "    Z -- the input of the activation function, also called pre-activation parameter \n",
    "    cache -- a python dictionary containing \"A\", \"W\" and \"b\" ; stored for computing the backward pass efficiently\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ### (≈ 1 line of code)\n",
    "    Z = np.dot(W,A)+b\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    assert(Z.shape == (W.shape[0], A.shape[1]))\n",
    "    cache = (A, W, b)\n",
    "    \n",
    "    return Z, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(Z):\n",
    "    a=1/(1+(np.exp(-Z)))\n",
    "    cache=Z\n",
    "    return a,cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def relu(Z):\n",
    "    a=np.maximum(np.zeros(Z.shape),Z)\n",
    "    cache=Z\n",
    "    return a,cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: linear_activation_forward\n",
    "\n",
    "def linear_activation_forward(A_prev, W, b, activation):\n",
    "    \"\"\"\n",
    "    Implement the forward propagation for the LINEAR->ACTIVATION layer\n",
    "\n",
    "    Arguments:\n",
    "    A_prev -- activations from previous layer (or input data): (size of previous layer, number of examples)\n",
    "    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
    "    b -- bias vector, numpy array of shape (size of the current layer, 1)\n",
    "    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
    "\n",
    "    Returns:\n",
    "    A -- the output of the activation function, also called the post-activation value \n",
    "    cache -- a python dictionary containing \"linear_cache\" and \"activation_cache\";\n",
    "             stored for computing the backward pass efficiently\n",
    "    \"\"\"\n",
    "    \n",
    "    if activation == \"sigmoid\":\n",
    "        # Inputs: \"A_prev, W, b\". Outputs: \"A, activation_cache\".\n",
    "        ### START CODE HERE ### (≈ 2 lines of code)\n",
    "        Z, linear_cache = linear_forward(A_prev,W,b)\n",
    "        A, activation_cache = sigmoid(Z)\n",
    "        ### END CODE HERE ###\n",
    "    \n",
    "    elif activation == \"relu\":\n",
    "        # Inputs: \"A_prev, W, b\". Outputs: \"A, activation_cache\".\n",
    "        ### START CODE HERE ### (≈ 2 lines of code)\n",
    "        Z, linear_cache = linear_forward(A_prev,W,b)\n",
    "        A, activation_cache = relu(Z)\n",
    "        ### END CODE HERE ###\n",
    "    elif activation ==\"linear\":\n",
    "        Z,linear_cache=linear_forward(A_prev,W,b)\n",
    "        A=Z\n",
    "        activation_cache=Z\n",
    "    \n",
    "    \n",
    "    assert (A.shape == (W.shape[0], A_prev.shape[1]))\n",
    "    cache = (linear_cache, activation_cache)\n",
    "\n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: L_model_forward\n",
    "\n",
    "def L_model_forward(X, parameters):\n",
    "    \"\"\"\n",
    "    Implement forward propagation for the [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID computation\n",
    "    \n",
    "    Arguments:\n",
    "    X -- data, numpy array of shape (input size, number of examples)\n",
    "    parameters -- output of initialize_parameters_deep()\n",
    "    \n",
    "    Returns:\n",
    "    AL -- last post-activation value\n",
    "    caches -- list of caches containing:\n",
    "                every cache of linear_relu_forward() (there are L-1 of them, indexed from 0 to L-2)\n",
    "                the cache of linear_sigmoid_forward() (there is one, indexed L-1)\n",
    "    \"\"\"\n",
    "\n",
    "    caches = []\n",
    "    A = X\n",
    "    L = len(parameters) // 2                  # number of layers in the neural network\n",
    "\n",
    "    # Implement [LINEAR -> RELU]*(L-1). Add \"cache\" to the \"caches\" list.\n",
    "    for l in range(1, L):\n",
    "        A_prev = A \n",
    "        ### START CODE HERE ### (≈ 2 lines of code)\n",
    "        A, cache = linear_activation_forward(A_prev,parameters[\"W\"+str(l)],parameters[\"b\"+str(l)],\"relu\") \n",
    "        caches.append(cache)\n",
    "        ### END CODE HERE ###\n",
    "    \n",
    "    # Implement LINEAR -> SIGMOID. Add \"cache\" to the \"caches\" list.\n",
    "    ### START CODE HERE ### (≈ 2 lines of code)\n",
    "    #AL, cache = linear_activation_forward(A,parameters[\"W\"+str(l+1)],parameters[\"b\"+str(l+1)],\"sigmoid\")\n",
    "    AL,cache=linear_activation_forward(A,parameters[\"W\"+str(l+1)],parameters[\"b\"+str(l+1)],\"linear\")\n",
    "    caches.append(cache)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    assert(AL.shape == (1,X.shape[1]))\n",
    "            \n",
    "    return AL, caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: compute_cost\n",
    "\n",
    "def compute_cost(AL, Y):\n",
    "    \"\"\"\n",
    "    Implement the cost function defined by equation (7).\n",
    "\n",
    "    Arguments:\n",
    "    AL -- probability vector corresponding to your label predictions, shape (1, number of examples)\n",
    "    Y -- true \"label\" vector (for example: containing 0 if non-cat, 1 if cat), shape (1, number of examples)\n",
    "\n",
    "    Returns:\n",
    "    cost -- cross-entropy cost\n",
    "    \"\"\"\n",
    "    \n",
    "    #m = Y.shape[1]\n",
    "\n",
    "    # Compute loss from aL and y.\n",
    "    ### START CODE HERE ### (≈ 1 lines of code)\n",
    "    #cost = -(np.sum(np.multiply(Y,np.log(AL))+np.multiply(1-Y,np.log(1-AL))))/m\n",
    "    cost=(((AL - Y) ** 2).mean())/2\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    cost = np.squeeze(cost)      # To make sure your cost's shape is what we expect (e.g. this turns [[17]] into 17).\n",
    "    assert(cost.shape == ())\n",
    "    \n",
    "\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: linear_backward\n",
    "\n",
    "def linear_backward(dZ, cache):\n",
    "    \"\"\"\n",
    "    Implement the linear portion of backward propagation for a single layer (layer l)\n",
    "\n",
    "    Arguments:\n",
    "    dZ -- Gradient of the cost with respect to the linear output (of current layer l)\n",
    "    cache -- tuple of values (A_prev, W, b) coming from the forward propagation in the current layer\n",
    "\n",
    "    Returns:\n",
    "    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
    "    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
    "    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
    "    \"\"\"\n",
    "    A_prev, W, b = cache\n",
    "    m = A_prev.shape[1]\n",
    "\n",
    "\n",
    "     ### START CODE HERE ### (≈ 3 lines of code)\n",
    "    dW = (np.dot(dZ,A_prev.T))/m\n",
    "    db = (np.sum(dZ,axis=1,keepdims=True))/m\n",
    "    dA_prev = np.dot(W.T,dZ)\n",
    "     ### END CODE HERE ###\n",
    "    assert (dA_prev.shape == A_prev.shape)\n",
    "    assert (dW.shape == W.shape)\n",
    "    assert (db.shape == b.shape) \n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid_backward(dA,activation_cache):\n",
    "    sig_der=np.dot(sigmoid(activation_cache),(1-sigmoid(activation_cache)))\n",
    "    dZ=np.dot(dA,sig_der)\n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def relu_backward(dA,activation_cache):\n",
    "    dZ = np.ones(activation_cache.shape)*dA\n",
    "    dZ[activation_cache < 0] = 0\n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def linear_act_backward(dA,activation_cache):\n",
    "    #linear_der=np.ones(activation_cache.shape)\n",
    "    dZ=dA\n",
    "    return dZ\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: linear_activation_backward\n",
    "\n",
    "def linear_activation_backward(dA, cache, activation):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for the LINEAR->ACTIVATION layer.\n",
    "    \n",
    "    Arguments:\n",
    "    dA -- post-activation gradient for current layer l \n",
    "    cache -- tuple of values (linear_cache, activation_cache) we store for computing backward propagation efficiently\n",
    "    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
    "    \n",
    "    Returns:\n",
    "    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
    "    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
    "    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
    "    \"\"\"\n",
    "    linear_cache, activation_cache = cache\n",
    "    \n",
    "    if activation == \"relu\":\n",
    "        ### START CODE HERE ### (≈ 2 lines of code)\n",
    "        dZ = relu_backward(dA,activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ,linear_cache)\n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "    elif activation == \"sigmoid\":\n",
    "        ### START CODE HERE ### (≈ 2 lines of code)\n",
    "        dZ = sigmoid_backward(dA,activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ,linear_cache)\n",
    "        ### END CODE HERE ###\n",
    "    elif activation == \"linear\":\n",
    "        dZ=linear_act_backward(dA,activation_cache)\n",
    "        dA_prev,dW,db=linear_backward(dZ,linear_cache)\n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: L_model_backward\n",
    "\n",
    "def L_model_backward(AL, Y, caches):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for the [LINEAR->RELU] * (L-1) -> LINEAR -> SIGMOID group\n",
    "    \n",
    "    Arguments:\n",
    "    AL -- probability vector, output of the forward propagation (L_model_forward())\n",
    "    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat)\n",
    "    caches -- list of caches containing:\n",
    "                every cache of linear_activation_forward() with \"relu\" (it's caches[l], for l in range(L-1) i.e l = 0...L-2)\n",
    "                the cache of linear_activation_forward() with \"sigmoid\" (it's caches[L-1])\n",
    "    \n",
    "    Returns:\n",
    "    grads -- A dictionary with the gradients\n",
    "             grads[\"dA\" + str(l)] = ... \n",
    "             grads[\"dW\" + str(l)] = ...\n",
    "             grads[\"db\" + str(l)] = ... \n",
    "    \"\"\"\n",
    "    grads = {}\n",
    "    L = len(caches) # the number of layers\n",
    "    m = AL.shape[1]\n",
    "    Y = Y.reshape(AL.shape) # after this line, Y is the same shape as AL\n",
    "    \n",
    "    # Initializing the backpropagation\n",
    "    ### START CODE HERE ### (1 line of code)\n",
    "    #dAL =- (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))\n",
    "    dAL=(AL-Y)/m\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Lth layer (SIGMOID -> LINEAR) gradients. Inputs: \"AL, Y, caches\". Outputs: \"grads[\"dAL\"], grads[\"dWL\"], grads[\"dbL\"]\n",
    "    ### START CODE HERE ### (approx. 2 lines)\n",
    "    current_cache = caches[L-1]\n",
    "    grads[\"dA\" + str(L)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] =linear_activation_backward(dAL,current_cache,\"linear\")\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    for l in reversed(range(L-1)):\n",
    "        # lth layer: (RELU -> LINEAR) gradients.\n",
    "        # Inputs: \"grads[\"dA\" + str(l + 2)], caches\". Outputs: \"grads[\"dA\" + str(l + 1)] , grads[\"dW\" + str(l + 1)] , grads[\"db\" + str(l + 1)] \n",
    "        ### START CODE HERE ### (approx. 5 lines)\n",
    "        current_cache = caches[l]\n",
    "        dA_prev_temp, dW_temp, db_temp = linear_activation_backward(grads[\"dA\"+str(l+2)],current_cache,\"relu\")\n",
    "        grads[\"dA\" + str(l + 1)] =dA_prev_temp\n",
    "        grads[\"dW\" + str(l + 1)] = dW_temp\n",
    "        grads[\"db\" + str(l + 1)] = db_temp\n",
    "        ### END CODE HERE ###\n",
    "\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: update_parameters\n",
    "\n",
    "def update_parameters(parameters, grads, learning_rate):\n",
    "    \"\"\"\n",
    "    Update parameters using gradient descent\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters \n",
    "    grads -- python dictionary containing your gradients, output of L_model_backward\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your updated parameters \n",
    "                  parameters[\"W\" + str(l)] = ... \n",
    "                  parameters[\"b\" + str(l)] = ...\n",
    "    \"\"\"\n",
    "    \n",
    "    L = len(parameters) // 2 # number of layers in the neural network\n",
    "\n",
    "    # Update rule for each parameter. Use a for loop.\n",
    "    ### START CODE HERE ### (≈ 3 lines of code)\n",
    "    for l in range(L):\n",
    "        parameters[\"W\"+str(l+1)]=parameters[\"W\"+str(l+1)]-np.multiply(learning_rate,grads[\"dW\"+str(l+1)])\n",
    "        parameters[\"b\"+str(l+1)]=parameters[\"b\"+str(l+1)]-np.multiply(learning_rate,grads[\"db\"+str(l+1)])\n",
    "    ### END CODE HERE ###\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### CONSTANTS ###\n",
    "layers_dims = [13, 64,64, 1] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: L_layer_model\n",
    "\n",
    "def L_layer_model(X, Y, layers_dims, learning_rate = 0.0075, num_iterations = 3000, print_cost=False):#lr was 0.009\n",
    "    \"\"\"\n",
    "    Implements a L-layer neural network: [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- data, numpy array of shape (number of examples, num_px * num_px * 3)\n",
    "    Y -- true \"label\" vector (containing 0 if cat, 1 if non-cat), of shape (1, number of examples)\n",
    "    layers_dims -- list containing the input size and each layer size, of length (number of layers + 1).\n",
    "    learning_rate -- learning rate of the gradient descent update rule\n",
    "    num_iterations -- number of iterations of the optimization loop\n",
    "    print_cost -- if True, it prints the cost every 100 steps\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- parameters learnt by the model. They can then be used to predict.\n",
    "    \"\"\"\n",
    "\n",
    "    np.random.seed(1)\n",
    "    costs = []                         # keep track of cost\n",
    "    \n",
    "    # Parameters initialization.\n",
    "    ### START CODE HERE ###\n",
    "    parameters = initialize_parameters_deep(layers_dims)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Loop (gradient descent)\n",
    "    for i in range(0, num_iterations):\n",
    "\n",
    "        # Forward propagation: [LINEAR -> RELU]*(L-1) -> LINEAR -> SIGMOID.\n",
    "        ### START CODE HERE ### (≈ 1 line of code)\n",
    "        AL, caches = L_model_forward(X,parameters)\n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "        # Compute cost.\n",
    "        ### START CODE HERE ### (≈ 1 line of code)\n",
    "        cost = compute_cost(AL,Y)\n",
    "        ### END CODE HERE ###\n",
    "    \n",
    "        # Backward propagation.\n",
    "        ### START CODE HERE ### (≈ 1 line of code)\n",
    "        grads = L_model_backward(AL,Y,caches)\n",
    "        ### END CODE HERE ###\n",
    " \n",
    "        # Update parameters.\n",
    "        ### START CODE HERE ### (≈ 1 line of code)\n",
    "        parameters = update_parameters(parameters,grads,learning_rate)\n",
    "        ### END CODE HERE ###\n",
    "                \n",
    "        # Print the cost every 100 training example\n",
    "        if print_cost and i % 100 == 0:\n",
    "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "        if print_cost and i % 100 == 0:\n",
    "            costs.append(cost)\n",
    "            \n",
    "    # plot the cost\n",
    "    plt.plot(np.squeeze(costs))\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('iterations (per tens)')\n",
    "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "    plt.show()\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "\n",
    "# We load the boston housing dataset\n",
    "\n",
    "data = np.loadtxt('boston_housing.txt')\n",
    "\n",
    "# We obtain the features and the targets\n",
    "\n",
    "X = data[ :, range(data.shape[ 1 ] - 1) ]\n",
    "y = data[ :, data.shape[ 1 ] - 1 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "permutation = np.random.choice(range(X.shape[ 0 ]),\n",
    "                        X.shape[ 0 ], replace = False)\n",
    "size_train =int( np.round(X.shape[ 0 ] * 0.9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "index_train = permutation[ 0 : size_train ]\n",
    "index_test = permutation[ size_train : ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = X[ index_train, : ]\n",
    "y_train = y[ index_train ]\n",
    "X_test = X[ index_test, : ]\n",
    "y_test = y[ index_test ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 302.933100\n",
      "Cost after iteration 100: 111.119544\n",
      "Cost after iteration 200: 47.241116\n",
      "Cost after iteration 300: 41.638183\n",
      "Cost after iteration 400: 39.745006\n",
      "Cost after iteration 500: 38.226802\n",
      "Cost after iteration 600: 37.123948\n",
      "Cost after iteration 700: 36.432646\n",
      "Cost after iteration 800: 35.770760\n",
      "Cost after iteration 900: 35.151935\n",
      "Cost after iteration 1000: 34.589741\n",
      "Cost after iteration 1100: 34.091171\n",
      "Cost after iteration 1200: 33.657185\n",
      "Cost after iteration 1300: 33.283681\n",
      "Cost after iteration 1400: 32.964189\n",
      "Cost after iteration 1500: 32.690617\n",
      "Cost after iteration 1600: 32.456922\n",
      "Cost after iteration 1700: 32.254146\n",
      "Cost after iteration 1800: 32.078433\n",
      "Cost after iteration 1900: 31.924255\n",
      "Cost after iteration 2000: 31.787687\n",
      "Cost after iteration 2100: 31.663566\n",
      "Cost after iteration 2200: 31.552219\n",
      "Cost after iteration 2300: 31.451012\n",
      "Cost after iteration 2400: 31.357127\n",
      "Cost after iteration 2500: 31.268792\n",
      "Cost after iteration 2600: 31.184690\n",
      "Cost after iteration 2700: 31.102198\n",
      "Cost after iteration 2800: 31.022134\n",
      "Cost after iteration 2900: 30.946581\n",
      "Cost after iteration 3000: 30.871777\n",
      "Cost after iteration 3100: 30.796480\n",
      "Cost after iteration 3200: 30.720813\n",
      "Cost after iteration 3300: 30.644287\n",
      "Cost after iteration 3400: 30.566369\n",
      "Cost after iteration 3500: 30.486699\n",
      "Cost after iteration 3600: 30.405450\n",
      "Cost after iteration 3700: 30.322092\n",
      "Cost after iteration 3800: 30.236386\n",
      "Cost after iteration 3900: 30.147214\n",
      "Cost after iteration 4000: 30.055510\n",
      "Cost after iteration 4100: 29.961145\n",
      "Cost after iteration 4200: 29.863488\n",
      "Cost after iteration 4300: 29.762011\n",
      "Cost after iteration 4400: 29.656932\n",
      "Cost after iteration 4500: 29.547690\n",
      "Cost after iteration 4600: 29.434029\n",
      "Cost after iteration 4700: 29.315502\n",
      "Cost after iteration 4800: 29.184997\n",
      "Cost after iteration 4900: 29.051974\n",
      "Cost after iteration 5000: 28.913160\n",
      "Cost after iteration 5100: 28.767976\n",
      "Cost after iteration 5200: 28.616084\n",
      "Cost after iteration 5300: 28.453412\n",
      "Cost after iteration 5400: 28.267929\n",
      "Cost after iteration 5500: 28.055381\n",
      "Cost after iteration 5600: 27.858758\n",
      "Cost after iteration 5700: 27.655789\n",
      "Cost after iteration 5800: 27.444244\n",
      "Cost after iteration 5900: 27.223512\n",
      "Cost after iteration 6000: 26.991720\n",
      "Cost after iteration 6100: 26.744198\n",
      "Cost after iteration 6200: 26.468958\n",
      "Cost after iteration 6300: 26.199992\n",
      "Cost after iteration 6400: 25.915869\n",
      "Cost after iteration 6500: 25.624203\n",
      "Cost after iteration 6600: 25.312438\n",
      "Cost after iteration 6700: 24.997574\n",
      "Cost after iteration 6800: 24.675285\n",
      "Cost after iteration 6900: 24.340779\n",
      "Cost after iteration 7000: 23.999298\n",
      "Cost after iteration 7100: 23.650529\n",
      "Cost after iteration 7200: 23.295160\n",
      "Cost after iteration 7300: 22.940370\n",
      "Cost after iteration 7400: 22.579704\n",
      "Cost after iteration 7500: 22.222560\n",
      "Cost after iteration 7600: 22.528413\n",
      "Cost after iteration 7700: 24.423978\n",
      "Cost after iteration 7800: 24.426049\n",
      "Cost after iteration 7900: 24.286597\n",
      "Cost after iteration 8000: 23.948812\n",
      "Cost after iteration 8100: 23.394070\n",
      "Cost after iteration 8200: 23.600829\n",
      "Cost after iteration 8300: 23.374023\n",
      "Cost after iteration 8400: 22.933844\n",
      "Cost after iteration 8500: 22.979230\n",
      "Cost after iteration 8600: 22.845403\n",
      "Cost after iteration 8700: 22.648467\n",
      "Cost after iteration 8800: 22.625340\n",
      "Cost after iteration 8900: 22.480797\n",
      "Cost after iteration 9000: 22.273519\n",
      "Cost after iteration 9100: 22.237973\n",
      "Cost after iteration 9200: 22.198657\n",
      "Cost after iteration 9300: 22.178884\n",
      "Cost after iteration 9400: 21.915273\n",
      "Cost after iteration 9500: 21.853904\n",
      "Cost after iteration 9600: 21.694158\n",
      "Cost after iteration 9700: 21.473625\n",
      "Cost after iteration 9800: 21.359115\n",
      "Cost after iteration 9900: 21.291975\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAVEAAAEWCAYAAADfK6SWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x24c4907c400>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "parameters = L_layer_model(X_train.T, y_train, layers_dims, num_iterations = 10000, print_cost = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'predict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-2b40f10cf910>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mpred_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'predict' is not defined"
     ]
    }
   ],
   "source": [
    "#pred_train = predict(train_x, train_y, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred_test = predict(X_test, y_test, parameters)"
   ]
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "neural-networks-deep-learning",
   "graded_item_id": "c4HO0",
   "launcher_item_id": "lSYZM"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
